#!/usr/bin/env ruby
# encoding: utf-8
# Usage: ticket profile
# Summary: Returns req/sec for the given log file
# Provide ticket completions
# main code handling lifted from support-docs repo

$LOAD_PATH.unshift File.join(ENV['_TICKET_ROOT'], 'share/ticket')

require 'clamp'
require 'helpers/ticket'
require 'date'
require 'mixlib/shellout'
require 'descriptive-statistics'



class ProfileCommand < Ticket::Command
  LOG_TYPES = %w{ solr expander hab_nginx nginx nginx2 erchef_requests rabbitmq_overview }
  DATETIME_FORMATS = {
    solr: {
      match: '^(.+?)\.\d+\s',
      divider: '_'
    },
    expander: {
      match: '^(.+?)\.\d+\s',
      divider: '_'
    },
    erchef_requests: {
      match: '^(.+?)Z\s*(.+)',
      divider: 'T'
    },
    nginx: {
      match: '\[(.+?)\]\s*(.+)',
      divider: ':'
    },
    nginx2: {
      match: '\[(.+?)\+(.+)\]\s*(.+)',
      divider: 'T'
    },
    hab_nginx: {
      match: '\[(\d{2}\/.+?)\]\s*(.+)',
      divider: ':'
    },
    rabbitmq_overview: {
      match: '\[(.+?)-\d+:\d+\]\s+(.+)',
      divider: nil
    }
  }

  option '--complete', :flag, 'autocomplete output', hidden: true
  option ['-m', '--minute'], :flag, 'show requests/minute'
  option ['-h', '--hour'], :flag, 'show requests/hour'
  option ['-M', '--avg-minute'], :flag, 'show avg requests/second for each minute window'
  option ['-f', '--filter'], 'filter', 'filter results using', attribute_name: :filter
  option ['-j', '--jq-value'], 'jq_value', 'jq string to pull values from json data', attribute_name: :jq_value
  option ['-a', '--accumulator'], :flag, 'values accumulate'
  option ['-s', '--summary'], :flag, 'print statistics at the end'
  option ['-d', '--detailed'], :flag, 'print detailed statistics at the end'
  option ['-t', '--type'], 'log_type', "log type to profile, options: [#{LOG_TYPES.join(', ')}]", attribute_name: :log_type do |s|
    unless complete?
      raise ArgumentError.new("Invalid log type specified '#{s}'") unless s.nil? or LOG_TYPES.include?(s)
    end
    s
  end
  parameter '[FILE] ...', 'files', :attribute_name => :files

  def record_details(line)
    @stats ||= {}
    case log_type
    when 'nginx', 'nginx2'
      nginx_details(line)
    end
  end

  def nginx_details(line)
    log_parts = line.scan(/"[^"]+"|\[[^\]]+\]|\S+/).map{ |s| s.delete('"') }.flatten.compact
    # puts log_parts.inspect

    request = log_parts[4]
    status = log_parts[5]
    method, path, http_version = request.split(' ')

    uri = URI(File.join("http://localhost", path))
    if uri.path.nil?
      endpoint = 'unknown'
    else
      path_parts = uri.path.split('/')
      if path_parts[1] == 'organizations'
        endpoint = path_parts[3]
        org = path_parts[2]
      else
        endpoint = path_parts.first
      end
    end
    # puts request.inspect

    # @stats['status'] ||= {}
    # @stats['status'][status.to_s] ||= 0
    # @stats['status'][status.to_s] += 1
    add_stat('status', status)
    add_stat('methods', method)
    add_stat('methods_status', method, status)
    add_stat('endpoints', endpoint)
    add_stat('endpoints_status', endpoint, status)
    add_stat('orgs', org)
  end

  def add_stat(group, first, second = nil, value = 1)
    return if first.nil?
    return if first.empty?

    @stats ||= {}
    @stats[group] ||= {}

    if second.nil?
      @stats[group][first] ||= 0
      @stats[group][first] += value
    else
      @stats[group][first] ||= {}
      @stats[group][first]['__total__'] ||= 0
      @stats[group][first]['__total__'] += value
      @stats[group][first][second] ||= 0
      @stats[group][first][second] += value
    end
  end

  def summarize(title, stats, max_key = 0)
    return if stats.keys.empty?

    total = stats.reject { |k,v| k == '__total__' }.values.sum.to_f

    max_key += stats.keys.map(&:length).max
    format = "%#{max_key}s: %7.3f%% (%i)"

    unless title.nil?
      puts "\n#{title}"
      puts '-' * 80
    end

    stats.sort_by{ |k,v| [-v,k] }.each do |key,value|
      next if key == '__total__'

      percent = value.to_f / total * 100
      puts format % [key, percent, value]
    end
    if stats.key?('__total__')
      puts '-' * 40
      puts format % ['Total', 100, stats['__total__']]
    end
  end

  def summarize_multi(title, stats)
    return if stats.keys.empty?

    total = stats.map{|k,v| v['__total__'] }.sum.to_f

    puts "\n#{title}"
    puts '=' * 80

    stats.sort_by{ |k,v| [-v['__total__'],k] }.each do |key,values|
      percent = values['__total__'].to_f / total * 100
      puts "%s - %0.3f%% (%i)" % [key, percent, values['__total__']]
      puts '-' * (40)
      summarize(nil, values)
      puts
    end
  end

  def display_details
    summarize "Orgs", @stats['orgs']
    summarize "HTTP Status", @stats['status']
    if detailed?
      summarize_multi "HTTP Method", @stats['methods_status']
      summarize_multi "HTTP Endpoints", @stats['endpoints_status']
    else
      summarize "HTTP Method", @stats['methods']
      summarize "HTTP Endpoints", @stats['endpoints']
    end
  end

  def execute
    return autocomplete if complete?

    granularity = :second
    units_label = 'r/s'
    if minute?
      granularity = :minute
      units_label = 'r/m'
    elsif hour?
      granularity = :hour
      units_label = 'r/h'
      units = 3600
    elsif avg_minute?
      granularity = :second_avg
      units_label = 'avg r/s'
      units = 60
    end

    # dup this so we don't get wierd side effects later on
    previous_accumulator_value = nil
    log_filter = filter.dup
    @requests = Hash.new { |hash, key| hash[key] = 0 }

    files.each do |file|
      gather_info(file, granularity)
    end

    print_chart @requests, granularity, units_label

    if summary? || detailed?
      display_details
      display_stats(@requests)
    end
  end

  def gather_info(file, granularity)
    time_match = DATETIME_FORMATS[log_type.to_sym][:match]
    time_divider = DATETIME_FORMATS[log_type.to_sym][:divider]

    case log_type
    when 'solr', 'expander'
      if log_filter.nil? && log_type == 'expander'
        log_filter = 'indexed'
      end
    when 'rabbitmq_overview'
      if accumulator?
        units_label = 'Î” msgs'
      else
        units_label = 'msgs'
      end
      units_label += '/s' if units > 1
    end
    signal_usage_error "Please specify a file to profile" if files.nil? || files.empty?

    time_regexp = Regexp.new(time_match)
    leftover_regexp = Regexp.new("#{time_match}(.+)")
    File.open(file, 'r') do |fp|
      fp.each do |line|
        next unless log_filter.nil? || line.match?(/#{log_filter}/)
        results = line.match(time_regexp)
        next if results.nil?

        value = if jq_value.nil?
          1
        else
          json = results[2]
          cmd = "echo '#{json}' | jq -r '#{jq_value}'"
          shellout = Mixlib::ShellOut.new(*cmd)
          shellout.run_command
          shellout.stdout.to_i
        end
        value = accumulate(value) if accumulator?
        # puts 'foo' if requests.nil?
        timestamp = date_glob(results[1], time_divider, granularity)

        record_details(line)

        @requests[timestamp] += value
      end
    end
  end

  def display_stats(requests)
    times = requests.keys.sort
    stats = DescriptiveStatistics::Stats.new(requests.values)
    format = '%15s: %s'
    puts "\nGeneral Stats"
    puts '-' * 80
    puts format % ['First time', times.first]
    puts format % ['Last time', times.last]
    puts format % ['Min value', stats.min]
    puts format % ['Max value', stats.max]
    (10..90).step(10).each do |i|
      puts format % ["#{i}th percentile", stats.value_from_percentile(i)]
    end
    puts "%15s: %0.3f%%" % ['STD Deviation', stats.standard_deviation]
    puts '-' * 80
  end

  def accumulate(value)
    # save previous value if not already done
    @previous ||= value

    delta = value - @previous
    @previous = value

    delta
  end

  def autocomplete
    lastargs = [ARGV.last, ARGV[-2]]
    if (lastargs.include?('-t') || lastargs.include?('--type')) && !LOG_TYPES.include?(ARGV.last)
      opts = LOG_TYPES.dup
    else
      opts = %w{ --type --filter --summary --detailed --accumulator --unit_time --help }
      # opts += Dir.glob('*')
    end
    opts -= ARGV
    print opts.join("\n")
    exit
  end

  private

  def date_glob(date, time_divider, granularity)
    d = time_divider.nil? ? date : date.sub(time_divider, ' ')

    case granularity
    when :hour
      results = d.match(/(.+):\d\d:\d\d/)
      d = results[1] unless results.nil?
      d += ':00'
    when :minute, :second_avg
      results = d.match(/(.+):\d\d/)
      d = results[1] unless results.nil?
    end

    d
  end

  def print_chart(requests, granularity, units_label)
    largest_requests_per_unit = requests.values.max
    timestamps = requests.keys.sort do |a, b|
      begin
        DateTime.parse(a) <=> DateTime.parse(b)
      rescue
        raise "Invalid date: #{a} or #{b}, if you are using `-t nginx` try `-t nginx2`"
      end
    end
    timestamps.each do |timestamp|
      requests_per_unit = requests[timestamp]
      printf "%s %6d #{units_label} [%-80s]\n", timestamp, requests_per_unit,
             '#' * (requests_per_unit.abs * 80 / largest_requests_per_unit)
    end
  end
end

ProfileCommand.run
